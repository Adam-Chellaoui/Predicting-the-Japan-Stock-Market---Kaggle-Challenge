{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20125,"status":"ok","timestamp":1655725804636,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"},"user_tz":-120},"id":"ch3xIpiQTWZX","outputId":"372952df-1a5e-4bf5-c4bb-988dc3045d74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQ5gcXWMNUMv"},"outputs":[],"source":["!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n","!tar -xzvf ta-lib-0.4.0-src.tar.gz\n","%cd ta-lib\n","!./configure --prefix=/usr\n","!make\n","!make install\n","!pip install Ta-Lib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQ187L4ATdYS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","mpl.style.use('seaborn')\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import seaborn as sns\n","import tensorflow as tf\n","from dataclasses import dataclass\n","import warnings\n","warnings.filterwarnings('ignore')\n","import time\n","\n","from scipy.stats import pearsonr\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from lightgbm import LGBMRegressor\n","import xgboost as xgb\n","from tqdm import tqdm\n","import talib as ta\n","import functools  "]},{"cell_type":"markdown","metadata":{"id":"Qe1LIqEAP13m"},"source":["### 1. Load, Clean, Create Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86zRQzGaUeVG"},"outputs":[],"source":["def load_data(supplement : bool) :\n","    stock_prices = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/train_files/stock_prices.csv', parse_dates=True)\n","    secondary_stock_prices = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/train_files/secondary_stock_prices.csv', parse_dates=True)\n","    options = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/train_files/options.csv', parse_dates=True)\n","    stock_list = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/stock_list.csv')\n","    if supplement :\n","        supplemental_stock_prices = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/supplemental_files/stock_prices.csv', parse_dates=True)\n","        supplemental_secondary_stock_prices = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/supplemental_files/secondary_stock_prices.csv', parse_dates=True)\n","        supplemental_options = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/supplemental_files/options.csv', parse_dates=True)\n","        \n","        stock_prices = stock_prices.append(supplemental_stock_prices)\n","        secondary_stock_prices = secondary_stock_prices.append(supplemental_secondary_stock_prices)\n","        options = options.append(supplemental_options)\n","    \n","    return stock_prices, secondary_stock_prices, options, stock_list\n","\n","def merge_stock_list(df : pd.DataFrame) :\n","    \n","    # On ajoute seulement la colonne 33 Sector Code qui donne le secteur de l'entreprise\n","    df = df.merge(stock_list[['SecuritiesCode','33SectorCode']], on='SecuritiesCode', how='left')\n","    df['33SectorCode'] = df['33SectorCode'].astype(int)\n","    return df\n","\n","def merge_stock_list_secondary(df : pd.DataFrame) :\n","    # On ajoute seulement la colonne 33 Sector Code qui donne le secteur de l'entreprise\n","    df = df.merge(stock_list[['SecuritiesCode','33SectorCode']], on='SecuritiesCode', how='left')\n","    df = df.drop(df[df['33SectorCode']=='-'].index)\n","    df['33SectorCode'] = df['33SectorCode'].astype(int)\n","    \n","    return df\n","\n","def preprocess_prices(df : pd.DataFrame) :\n","    \n","    # Cast the categorical data\n","    df['SecuritiesCode'] = df['SecuritiesCode'].astype('category')\n","    df['AdjustmentFactor'] = df['AdjustmentFactor'].astype('category')\n","    df['SupervisionFlag'] = df['SupervisionFlag'].map({True: 1, False: 0})\n","    \n","    # Remove useless column\n","    df.drop(columns = ['RowId'], axis=1, inplace=True)\n","    \n","    # Manage NaN values\n","    df['ExpectedDividend'] = df['ExpectedDividend'].fillna(0)\n","    df.dropna(inplace=True) # Il reste 8200 rows contenant que des Nan sur les colonnes OLHC, on les drop\n","    \n","    # Transform date into int\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df['DateInt'] = df['Date'].dt.strftime(\"%Y%m%d\").astype(int)\n","    \n","    return df\n","\n","def preprocess_prices_submission(df : pd.DataFrame) :\n","    \n","    # Cast the categorical data\n","    df['SecuritiesCode'] = df['SecuritiesCode'].astype('category')\n","    df['AdjustmentFactor'] = df['AdjustmentFactor'].astype('category')\n","    df['SupervisionFlag'] = df['SupervisionFlag'].map({True: 1, False: 0})\n","    \n","    # Remove useless column\n","    df.drop(columns = ['RowId'], axis=1, inplace=True)\n","    \n","    # Manage NaN values\n","    df['ExpectedDividend'] = df['ExpectedDividend'].fillna(0)\n","    \n","    # Transform date into int\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df['DateInt'] = df['Date'].dt.strftime(\"%Y%m%d\").astype(int)\n","    \n","    return df\n","\n","def fillna_prices_submission(df : pd.DataFrame) :\n","    df = df.sort_values(by=['SecuritiesCode']).fillna(method='backfill')\n","    return df\n","\n","def add_secondary_market_features (df_prices : pd.DataFrame, df_secondary : pd.DataFrame) :\n","    df_prices = merge_stock_list(df_prices)\n","    df_secondary = merge_stock_list_secondary(df_secondary)\n","    df_all_securities = df_prices.append(df_secondary)\n","    date_sector_volume = df_all_securities.groupby(by=['Date','33SectorCode'])['Volume'].sum()\n","    date_sector_volume = date_sector_volume.reset_index()\n","    date_sector_volume = date_sector_volume.rename(columns = {'Volume' : 'VolumeSector'})\n","    del df_all_securities\n","    df_prices = df_prices.merge(date_sector_volume, on=['Date', '33SectorCode'], how='outer')\n","    return df_prices\n","\n","def add_options_features(df_prices : pd.DataFrame, df_options : pd.DataFrame) :\n","    df_options['Date'] = pd.to_datetime(df_options['Date'])\n","    put_call_day_volume = df_options.groupby(['Date','Putcall'])['TradingVolume'].sum().unstack().rename(columns={1:'Put',2:'Call'})\n","    put_call_day_volume['put/call'] = put_call_day_volume['Put']/put_call_day_volume['Call']\n","    put_call_day_volume['Volatility'] = df_options.groupby('Date')['ImpliedVolatility'].mean()\n","    df_prices = df_prices.merge(put_call_day_volume[['put/call', 'Volatility']], how='left', on='Date')\n","    return df_prices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-FHjuwPQgGX"},"outputs":[],"source":["df_prices, df_secondary, df_options, stock_list = load_data(1)\n","df_prices = preprocess_prices(df_prices)\n","df_secondary = preprocess_prices(df_secondary)\n","df_prices = add_secondary_market_features (df_prices, df_secondary)\n","df_prices = add_options_features(df_prices, df_options)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-9xxvVhQ8Om"},"outputs":[],"source":["def get_talib_features(df):\n","    \"\"\"\n","    Get technical features from TA-Lib\n","    \"\"\"\n","    op = df['Open']\n","    hi = df['High']\n","    lo = df['Low']\n","    cl = df['Close']\n","    vo = df['Volume']\n","    \n","    # Overlap Studies\n","    df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n","    df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n","    df['EMA_30'] = ta.EMA(cl, timeperiod=30)\n","    df['EMA_20'] = ta.EMA(cl, timeperiod=20)\n","    df['EMA_10'] = ta.EMA(cl, timeperiod=10)\n","    df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n","    # df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n","    # df['MA_50'] = ta.MA(cl, timeperiod=50, matype=0)\n","    # df['MA_30'] = ta.MA(cl, timeperiod=30, matype=0)\n","    # df['MA_15'] = ta.MA(cl, timeperiod=15, matype=0)\n","    # df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n","    # df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n","    # df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n","    # df['SMA'] = ta.SMA(cl, timeperiod=30)\n","    # df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n","    df['TEMA_50'] = ta.TEMA(df['Close'], timeperiod=50)\n","    df['TEMA_30'] = ta.TEMA(df['Close'], timeperiod=30)\n","    df['TEMA_15'] = ta.TEMA(df['Close'], timeperiod=15)\n","    df['TEMA_10'] = ta.TEMA(df['Close'], timeperiod=10)\n","    # df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n","    # df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n","    \n","    # Momentum Indicators\n","    df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n","    df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n","    df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n","    # df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n","    # df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n","    # df['BOP'] = ta.BOP(op, hi, lo, cl)\n","    # df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n","    # df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n","    df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n","    # df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n","    # df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n","    # df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n","    # df['MOM'] = ta.MOM(cl, timeperiod=10)\n","    # df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n","    # df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n","    df['RSI'] = ta.RSI(cl, timeperiod=14)\n","    # df['STOCH_slowk'], df['STOCH_slowd'] = ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n","    # df['STOCHF_fastk'], df['STOCHF_fastd'] = ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)\n","    # df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n","    # df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n","    # df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n","    # df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n","    \n","    # Volume Indicators\n","    df['AD'] = ta.AD(hi, lo, cl, vo)\n","    df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n","    df['OBV'] = ta.OBV(cl, vo)\n","    \n","    # Volatility Indicators\n","    df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14)\n","    df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14)\n","    df['TRANGE'] = ta.TRANGE(hi, lo, cl)\n","    \n","    # Cycle Indicators\n","    # df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n","    # df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n","    # df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n","    # df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n","    # df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n","    \n","    # Statistic Functions\n","    # df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n","    # df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n","    # df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n","    # df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n","    # df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n","    # df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n","    # df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1)   \n","    \n","    return df\n","\n","def add_features_security(df : pd.DataFrame, security : int):\n","    \"\"\"\n","    Args:\n","        price (pd.DataFrame)  : pd.DataFrame include stock_price\n","        security (int)  : A local code for a listed company\n","    Returns:\n","        feature DataFrame (pd.DataFrame)\n","    \"\"\"\n","    data_security = df[df.SecuritiesCode == security].copy()\n","    \n","    # Adds all 42 features\n","    data_security = get_talib_features(data_security)\n","    \n","    # filling data for nan and inf\n","    data_security = data_security.fillna(method='ffill')\n","    data_security = data_security.replace([np.inf, -np.inf, np.nan], 0)\n","\n","    return data_security\n","\n","def add_features_dataframe(df : pd.DataFrame) :\n","    codes = sorted(df[\"SecuritiesCode\"].unique())\n","    buff = []\n","    for code in tqdm(codes):\n","        security_features = add_features_security(df, code)\n","        buff.append(security_features)\n","    df_augmented = pd.concat(buff)\n","    \n","    return df_augmented\n","\n","def drop_null_values_lagged(df) :\n","  codes = sorted(df[\"SecuritiesCode\"].unique())\n","  buff = []\n","  for code in tqdm(codes):\n","    data_security = df[df.SecuritiesCode == code].copy()\n","    data_security.reset_index(drop=True, inplace=True)\n","    idx_drop = data_security[features_nodate].ne(0).idxmax().max()\n","    data_security.drop(data_security.iloc[:idx_drop].index, inplace=True)\n","    buff.append(data_security)\n","  df_clean = pd.concat(buff)\n","  \n","  return df_clean"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41755,"status":"ok","timestamp":1655726147664,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"},"user_tz":-120},"id":"_nmXUi2YRIhX","outputId":"f9e0cea9-027a-4478-c773-052cb0fef86d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [00:39<00:00, 50.45it/s]\n"]}],"source":["df_prices = add_features_dataframe(df_prices)"]},{"cell_type":"markdown","metadata":{"id":"oQDkelpASFlJ"},"source":["### 2. Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdOezPvbRMQ0"},"outputs":[],"source":["# features_training = [\n","#     # OLHCV\n","#     \"Open\",\n","#     \"High\",\n","#     \"Low\",\n","#     \"Close\",\n","#     \"Volume\",\n","#     # New\n","#     \"put/call\",\n","#     \"Volatility\",\n","#     \"VolumeSector\",\n","#     # Overlap Studies\n","#     \"BBANDS_upper\",\n","#     \"BBANDS_lower\",\n","#     \"EMA_10\",\n","#     \"EMA_20\",\n","#     \"EMA_30\",\n","#     #\"SMA\",\n","#     #\"TEMA_15\",\n","#     #\"TEMA_30\",\n","#     #\"HT_TRENDLINE\",\n","#     # Momentum Indicators\n","#     \"MACD_macd\",\n","#     \"RSI\",\n","#     \"APO\",\n","#     \"ADX\",\n","#     # Volume Indicators\n","#     \"AD\",\n","#     \"OBV\",\n","#     # Volatility Indicators\n","#     \"ATR\",\n","#     # Cycle Indicators\n","#     #\"HT_TRENDMODE\",\n","#     \"Date\",\n","#     \"Target\"\n","# ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JctSglY1_9ef"},"outputs":[],"source":["features_training = [\n","    \"SecuritiesCode\",\n","    # OLHCV\n","    \"Open\",\n","    \"High\",\n","    \"Low\",\n","    \"Close\",\n","    \"Volume\",\n","    #\"AdjustmentFactor\",\n","    #\"ExpectedDividend\",\n","    # New\n","    # Overlap Studies\n","    \"BBANDS_upper\",\n","    \"BBANDS_lower\",\n","    \"EMA_20\",\n","    \"TEMA_15\",\n","    \"TEMA_30\",\n","    \"TEMA_50\",\n","    \"HT_TRENDLINE\",\n","    # Momentum Indicators\n","    \"MACD_macd\",\n","    \"RSI\",\n","    \"APO\",\n","    \"ADX\",\n","    # Volume Indicators\n","    \"AD\",\n","    \"OBV\",\n","    # Volatility Indicators\n","    \"NATR\",\n","    \"put/call\",\n","    \"VolumeSector\",\n","    ######\n","    \"Date\",\n","    \"Target\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLHA9QvWRSwK"},"outputs":[],"source":["features_nodate = features_training.copy()\n","features_nodate.remove(\"Date\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4vaPJFmYOy4"},"outputs":[],"source":["features_nodate_nocode = features_nodate.copy()\n","features_nodate_nocode.remove(\"SecuritiesCode\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25529,"status":"ok","timestamp":1655726173189,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"},"user_tz":-120},"id":"7lHvJayNDCMR","outputId":"e47ad2dc-9243-4970-9405-2eb4238b648c"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [00:22<00:00, 87.11it/s] \n"]}],"source":["df_prices = drop_null_values_lagged(df_prices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBE08Vp4R30I"},"outputs":[],"source":["securities = df_prices.SecuritiesCode.astype(int).unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQRq1NMHSRxU"},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJl7aAkWSSST"},"outputs":[],"source":["# Save all \"global\" variables within the G class (G stands for global)\n","@dataclass\n","class G:\n","    TRAIN_END = \"2022-04-30\"\n","    TEST_START = \"2022-05-01\"\n","    WINDOW_SIZE = 30\n","    BATCH_SIZE = 32\n","    SHUFFLE_BUFFER_SIZE = 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sZqdQ0ko4tD"},"outputs":[],"source":["features_to_scale = features_nodate_nocode.copy()\n","features_to_scale.remove(\"put/call\")\n","features_to_scale.remove(\"VolumeSector\")\n","# features_to_scale.remove(\"Target\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e__f1pPwSVRN"},"outputs":[],"source":["def windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE):\n","    # Create dataset from the series\n","    dataset = tf.data.Dataset.from_tensor_slices(series)\n","    \n","    # Slice the dataset into the appropriate windows\n","    dataset = dataset.window(window_size + 1,shift = 1, drop_remainder = True)\n","    \n","    # Flatten the dataset\n","    dataset = dataset.flat_map(lambda window : window.batch(window_size+1))\n","    \n","    # Suffle to reduce biais\n","    dataset = dataset.shuffle(shuffle_buffer)\n","    \n","    # Split it into the features and labels\n","    dataset = dataset.map(lambda window : (window[:-1, :-1], window[-1, -1]))\n","    \n","    # Batch it\n","    dataset = dataset.batch(batch_size)\n","    \n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPv_hZ-XScaf"},"outputs":[],"source":["def create_uncompiled_model():\n","\n","    model = tf.keras.models.Sequential([ \n","        tf.keras.layers.LSTM(64, activation='tanh', return_sequences=False),\n","        tf.keras.layers.Dense(32, activation='gelu'),\n","        # tf.keras.layers.Dropout(0.1),\n","        # tf.keras.layers.LSTM(50, activation='tanh', return_sequences=False),\n","        # tf.keras.layers.Dropout(0.1),\n","        tf.keras.layers.Dense(1, activation='linear')\n","    ]) \n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqLX5oygBQgN"},"outputs":[],"source":["def create_big_dataset(df_prices) :\n","\n","  scaler_global_variables = MinMaxScaler()\n","  global_variables = ['put/call', 'VolumeSector']\n","  scaler_global_variables.fit(df_prices[global_variables])\n","  globals()[\"global_scaler\"] = scaler_global_variables\n","\n","  full_dataset_train = None\n","  full_dataset_test = None\n","  for security in tqdm(securities) :\n","    df_security = df_prices[df_prices.SecuritiesCode==security]\n","    df_security.reset_index(drop=True, inplace=True)\n","    # idx_drop = df_security[features_nodate].ne(0).idxmax().max() # last index where a value is zero because of the lagged features\n","    # # so we drop everything before :\n","    # df_security.drop(df_security.iloc[:idx_drop].index, inplace=True)\n","\n","    # On utilise MinMax scale car les features ne suivent pas de distribution normale, et qu'il n'y a pas de outlier\n","    scaler = MinMaxScaler()\n","    df_security_scaled = scaler.fit_transform(df_security[features_to_scale])\n","    df_security_scaled = pd.DataFrame(data=df_security_scaled, columns=features_to_scale)\n","\n","    df_date = df_security.Date # on ne l'enregistre pas en variable globale, elle sera copiée et n'a pas d'importance\n","    df_date.reset_index(inplace=True, drop=True)\n","    df_security_scaled['Date'] = df_date\n","    df_code = df_security.SecuritiesCode # on ne l'enregistre pas en variable globale, elle sera copiée et n'a pas d'importance\n","    df_code.reset_index(inplace=True, drop=True)\n","    df_security_scaled['SecuritiesCode'] = df_code\n","\n","    df_global_variables = scaler_global_variables.transform(df_security[global_variables])\n","    df_global_variables = pd.DataFrame(data=df_global_variables, columns=global_variables)\n","    df_security_scaled = pd.concat([df_global_variables, df_security_scaled], axis=1) \n","    string_security_scaled = \"df_security_{}_scaled\".format(security)\n","    globals()[string_security_scaled] = df_security_scaled\n","\n","    # On enregistre le min et le max des features pour pouvoir inverse scale lors de la prediction\n","    string_min_target = \"min_target_{}\".format(security)\n","    string_max_target = \"max_target_{}\".format(security)\n","    globals()[string_min_target] = scaler.data_min_[-1]\n","    globals()[string_max_target] = scaler.data_max_[-1]\n","\n","    # On split pour garder que le train\n","    df_train = df_security_scaled[df_security_scaled['Date'] <= G.TRAIN_END][features_nodate_nocode]\n","    df_test = df_security_scaled[df_security_scaled['Date'] >= G.TEST_START][features_nodate_nocode]\n","\n","    tf_dataset_train = windowed_dataset(df_train)\n","    tf_dataset_test = windowed_dataset(df_test)    \n","\n","    # On merge \n","    if security==1301 :  # On crée le dataset à partir de la première security\n","      full_dataset_train = tf_dataset_train\n","      full_dataset_test = tf_dataset_test\n","    else :                # Puis on ajoute les autres\n","      full_dataset_train = tf.data.Dataset.concatenate(full_dataset_train, tf_dataset_train)\n","      full_dataset_test = tf.data.Dataset.concatenate(full_dataset_test, tf_dataset_test)\n","  \n","  return full_dataset_train, full_dataset_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179036,"status":"ok","timestamp":1655726352476,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"},"user_tz":-120},"id":"-VxTQAXiKEkn","outputId":"78d105c2-3d96-4ced-b303-b1d378e486e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [02:58<00:00, 11.20it/s]\n"]}],"source":["full_dataset_train, full_dataset_test = create_big_dataset(df_prices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KT359E4FOKI"},"outputs":[],"source":["filepath = \"/content/drive/My Drive/Colab Notebooks/saved_models/checkpoint_4\"\n","my_callbacks = tf.keras.callbacks.ModelCheckpoint(filepath,  \n","                                     verbose=0, \n","                                     save_best_only=False,\n","                                     save_weights_only=False, \n","                                     mode='auto', \n","                                     save_freq='epoch')"]},{"cell_type":"code","source":["model = tf.keras.models.load_model(\"/content/drive/My Drive/Colab Notebooks/saved_models/model_full_dataset_1epoch_targetlocal\")"],"metadata":{"id":"8zLz2ruABqyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcADVzaTS6t-","executionInfo":{"status":"ok","timestamp":1655737510218,"user_tz":-120,"elapsed":10376998,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"}},"outputId":"bb58ad68-2414-46a9-efba-365793decffb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","69454/69454 [==============================] - 2872s 41ms/step - loss: 0.0062 - mae: 0.0839\n","Epoch 2/3\n","69454/69454 [==============================] - 2853s 41ms/step - loss: 0.0061 - mae: 0.0836\n","Epoch 3/3\n","69454/69454 [==============================] - 3103s 45ms/step - loss: 0.0061 - mae: 0.0831\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/saved_models/model_full_dataset_4epoch_targetlocal/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/saved_models/model_full_dataset_4epoch_targetlocal/assets\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff316930210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]}],"source":["# model = create_uncompiled_model()\n","# model.compile(loss=tf.keras.losses.Huber(),\n","#               optimizer=optimizer, \n","#               metrics=[\"mae\"],\n","#               )\n","history = model.fit(full_dataset_train, epochs=3, validation_data=full_dataset_test)\n","model.save(\"/content/drive/My Drive/Colab Notebooks/saved_models/model_full_dataset_4epoch_targetlocal\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-s48u-aVoGtN"},"outputs":[],"source":["model = tf.keras.models.load_model(filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BEK5MiPA1Yf"},"outputs":[],"source":["model = tf.keras.models.load_model(\"/content/drive/My Drive/Colab Notebooks/saved_models/model_full_dataset_1epoch_targetlocal\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ij2-zgfIgx0U"},"outputs":[],"source":["def rescale_min_max(value, min_, max_) :\n","    return value * (max_ - min_) + min_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McRJD5RWxQIO"},"outputs":[],"source":["def rescale_std(value, mean, std) :\n","  return value * std + mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8RRt8-hkwl6"},"outputs":[],"source":["# Define the function to return the SMAPE value\n","def calculate_smape(actual, predicted) -> float:\n","  \n","    # Convert actual and predicted to numpy\n","    # array data type if not already\n","    if not all([isinstance(actual, np.ndarray), isinstance(predicted, np.ndarray)]):\n","        actual, predicted = np.array(actual), np.array(predicted)\n","  \n","    return round(\n","        np.mean(np.abs(predicted - actual) / \n","            ((np.abs(predicted) + np.abs(actual))/2))*100, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFUS0ZZkg4Dg"},"outputs":[],"source":["def compute_metrics(true_series, forecast):\n","    mse = mean_squared_error(true_series, forecast)\n","    mae = mean_absolute_error(true_series, forecast)\n","    mape = mean_absolute_percentage_error(true_series, forecast)\n","    smape = calculate_smape(true_series, forecast)\n","    r = pearsonr(true_series, forecast)\n","    print(f\"mse: {mse:.5f}, mae: {mae:.5f}, mape: {mape:.5f},  smape: {smape:.5f}, pearsoncorr: {r[0]:.3f} for forecast \\n\")\n","    return mse, mae, mape, r, smape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yy2uHOGLaCO_"},"outputs":[],"source":["def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n","    \"\"\"\n","    Args:\n","        df (pd.DataFrame): predicted results\n","        portfolio_size (int): # of equities to buy/sell\n","        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n","    Returns:\n","        (float): sharpe ratio\n","    \"\"\"\n","    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): predicted results\n","            portfolio_size (int): # of equities to buy/sell\n","            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n","        Returns:\n","            (float): spread return\n","        \"\"\"\n","        assert df['Rank'].min() == 0\n","        assert df['Rank'].max() == len(df['Rank']) - 1\n","        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n","        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n","        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n","        return purchase - short\n","\n","    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n","    sharpe_ratio = buf.mean() / buf.std()\n","    return sharpe_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-yunGZ_MJs1"},"outputs":[],"source":["def calc_spread_average_return(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n","    \"\"\"\n","    Args:\n","        df (pd.DataFrame): predicted results\n","        portfolio_size (int): # of equities to buy/sell\n","        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n","    Returns:\n","        (float): sharpe ratio\n","    \"\"\"\n","    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): predicted results\n","            portfolio_size (int): # of equities to buy/sell\n","            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n","        Returns:\n","            (float): spread return\n","        \"\"\"\n","        assert df['Rank'].min() == 0\n","        assert df['Rank'].max() == len(df['Rank']) - 1\n","        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n","        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n","        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n","        return purchase - short\n","\n","    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n","    sharpe_ratio = buf.mean()\n","    return sharpe_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0OCdVXEaOuY"},"outputs":[],"source":["def set_rank_date(df):\n","    \"\"\"\n","    Args:\n","        df (pd.DataFrame): df including predict_column for a Date\n","    Returns:\n","        df (pd.DataFrame): df with Rank for a day\n","    \"\"\" \n","    # sort records to set Rank\n","    df = df.sort_values(\"TargetPredicted\", ascending=False)\n","    # set Rank starting from 0\n","    df[\"Rank\"] = np.arange(len(df[\"Target\"]))\n","    return df\n","\n","def set_rank_prediction(df) :\n","        \n","    df['Target'] = pred_model.predict(df.drop('Date', axis=1))\n","    df = df.sort_values([\"Date\", \"Target\"], ascending=[True, False])\n","    df = df.groupby(\"Date\").apply(set_rank_date)\n","    df = df.reset_index(drop=True)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJfui5a7BGxl"},"outputs":[],"source":["def make_predictions_security(security) :\n","  string_security_scaled = \"df_security_{}_scaled\".format(security)\n","  df = globals()[string_security_scaled]\n","  df.reset_index(drop=True, inplace=True)\n","  first_idx_test = (df['Date'] >= G.TEST_START).idxmax() - 30\n","  if first_idx_test<=0 :  # Si il n' a pas de date à predire pour la période de test\n","    return 0\n","  last_idx_test = df.shape[0] # en fait il faut faire -1 pour avoir le dernier indice du dataframe\n","  df_window = windowed_dataset(df.iloc[first_idx_test:][features_nodate_nocode])\n","\n","  predictions = df.iloc[first_idx_test + 30:][['Date', 'Target']]\n","\n","  forecast = model.predict(df_window)\n","\n","  # Rescale target\n","  string_min_target = \"min_target_{}\".format(security)\n","  string_max_target = \"max_target_{}\".format(security)\n","  min = globals()[string_min_target]\n","  max = globals()[string_max_target]\n","  # rescale = functools.partial(rescale_min_max, min_=min, max_=max)\n","  # forecasts = list(map(rescale, forecasts))\n","  # targets = list(map(rescale, targets))\n","  # min = globals()[\"global_scaler\"].data_min_[-1]\n","  # max = globals()[\"global_scaler\"].data_max_[-1]\n","\n","  predictions['TargetPredicted'] = forecast\n","\n","  predictions.Target = predictions.Target.apply(lambda x :rescale_min_max(x, min, max))\n","  predictions.TargetPredicted = predictions.TargetPredicted.apply(lambda x :rescale_min_max(x, min, max))\n","  # # We output in a dataframe\n","  # predictions = {'Date':days,'TargetPredicted':forecasts, 'Target' : targets}\n","  # predictions = pd.DataFrame(predictions)\n","\n","  return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuH4gWGFZuNk"},"outputs":[],"source":["# def make_predictions_security(security) :\n","#   string_security_scaled = \"df_security_{}_scaled\".format(security)\n","#   df = globals()[string_security_scaled]\n","#   df.reset_index(drop=True, inplace=True)\n","#   first_idx_test = (df['Date'] >= G.TEST_START).idxmax()\n","#   if first_idx_test==0 :  # Si il n' a pas de date à predire pour la période de test\n","#     return 0\n","#   last_idx_test = df.shape[0] # en fait il faut faire -1 pour avoir le dernier indice du dataframe\n","\n","#   days = []\n","#   forecasts = []\n","#   targets = []\n","#   for day in range(first_idx_test, last_idx_test) :\n","#     df_window = df.iloc[day-30:day].drop(columns=['Date', 'Target'])\n","#     tf_window = tf.convert_to_tensor(df_window)\n","#     forecast = model.predict(tf.expand_dims(tf_window, axis=0)).astype(float)[0,0]\n","#     forecasts.append(forecast)\n","#     days.append(df.iloc[day].Date)\n","#     targets.append(df.iloc[day].Target)\n","\n","#   # Rescale target\n","#   string_min_target = \"min_target_{}\".format(security)\n","#   string_max_target = \"max_target_{}\".format(security)\n","#   min = globals()[string_min_target]\n","#   max = globals()[string_max_target]\n","#   rescale = functools.partial(rescale_min_max, min_=min, max_=max)\n","#   forecasts = list(map(rescale, forecasts))\n","#   targets = list(map(rescale, targets))\n","\n","#   # We output in a dataframe\n","#   predictions = {'Date':days,'TargetPredicted':forecasts, 'Target' : targets}\n","#   predictions = pd.DataFrame(predictions)\n","\n","#   return predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iegofiMooZ_e"},"outputs":[],"source":["def add_ranking(df) :\n","  # Predicts Target for securities over all the Test period\n","  securities = sorted(df.SecuritiesCode.unique())\n","  buff = []\n","  for security in tqdm(securities) :\n","    pred_security = make_predictions_security(security)\n","    if type(pred_security) == int : continue\n","    else :\n","      buff.append(pred_security)\n","  df = pd.concat(buff)\n","  df = df.sort_values([\"Date\", \"TargetPredicted\"], ascending=[True, False])\n","  df = df.groupby(\"Date\").apply(set_rank_date)\n","  df = df.reset_index(drop=True)\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369491,"status":"ok","timestamp":1655737895311,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"},"user_tz":-120},"id":"WrQQlUisQApg","outputId":"033dcdae-6822-4eed-ecb7-65fa2104fc49"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [06:08<00:00,  5.42it/s]\n"]}],"source":["predictions = add_ranking(df_prices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eD7fWT1aqN1m"},"outputs":[],"source":["predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211,"status":"ok","timestamp":1655737940927,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"},"user_tz":-120},"id":"fBXoVQZkkanP","outputId":"2c377019-5a1c-4c34-a98d-606b483c2d45"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5526647957949242"]},"metadata":{},"execution_count":58}],"source":["calc_spread_return_sharpe(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1655737895560,"user":{"displayName":"ADAM CHELLAOUI","userId":"09326746463054769268"},"user_tz":-120},"id":"O4GsB8xRWjQG","outputId":"24840649-4e07-49cf-af07-9d6d61669269"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4187879334893368"]},"metadata":{},"execution_count":57}],"source":["calc_spread_average_return(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KjWNjwl5RKHM"},"outputs":[],"source":["predicted_rank.to_pickle(\"/content/drive/My Drive/Colab Notebooks/predicted_rank.pkl\")  "]}],"metadata":{"colab":{"collapsed_sections":[],"name":"LSTM_FullDataset_Train.ipynb","provenance":[],"authorship_tag":"ABX9TyPuKxm8zVRsa78Nv2xVafNj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}